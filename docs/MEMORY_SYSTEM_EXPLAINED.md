# Memory Retrieval System - Explained

## ðŸŽ¯ TL;DR: How It Works Now

When you send Nicky a message like **"Hey Nicky, what's the story with Sal the butcher?"**, here's what happens:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER MESSAGE                                                â”‚
â”‚  "Hey Nicky, what's the story with Sal the butcher?"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Keyword Extraction (10ms)                          â”‚
â”‚  â†’ ["nicky", "story", "sal", "butcher"]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Contextual Enhancement (50ms)                      â”‚
â”‚  + Recent conversation keywords                              â”‚
â”‚  + Personality keywords (if Story Mode: +family, +newark)   â”‚
â”‚  + Mode keywords (if PODCAST: +episode, +show)              â”‚
â”‚  + Emotion keywords (angry â†’ +frustration)                  â”‚
â”‚  â†’ ["nicky", "story", "sal", "butcher", "family", "newark"] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Parallel Context Gathering (400-700ms)             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ ðŸ”¢ HYBRID SEARCH    â”‚  â”‚ ðŸ“„ DOCUMENTS       â”‚          â”‚
â”‚  â”‚ (300ms)             â”‚  â”‚ (50ms)             â”‚          â”‚
â”‚  â”‚                     â”‚  â”‚                     â”‚          â”‚
â”‚  â”‚ A. Semantic Search: â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”‚    1. Generate      â”‚                                    â”‚
â”‚  â”‚       embedding     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚       (200ms)       â”‚  â”‚ ðŸ“š LORE CONTEXT    â”‚          â”‚
â”‚  â”‚    2. Vector search â”‚  â”‚ (50ms)             â”‚          â”‚
â”‚  â”‚       4,136 memoriesâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”‚       (100ms)       â”‚                                    â”‚
â”‚  â”‚                     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ B. Keyword Search:  â”‚  â”‚ ðŸŽ“ TRAINING        â”‚          â”‚
â”‚  â”‚    SQL LIKE queries â”‚  â”‚    EXAMPLES        â”‚          â”‚
â”‚  â”‚    (50ms)           â”‚  â”‚ (100ms)            â”‚          â”‚
â”‚  â”‚                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”‚ C. Combine & rank   â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                           â”‚ ðŸŽ™ï¸  PODCAST        â”‚          â”‚
â”‚  All running in parallel! â”‚    MEMORIES        â”‚          â”‚
â”‚                           â”‚ (50ms)             â”‚          â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Scoring & Filtering (20ms)                         â”‚
â”‚                                                              â”‚
â”‚  For EACH memory retrieved:                                 â”‚
â”‚    1. Base Score = semantic_similarity * 1.2                â”‚
â”‚                  + importance * 0.1                          â”‚
â”‚                  + confidence * 0.001                        â”‚
â”‚                                                              â”‚
â”‚    2. Contextual Relevance = 0.5 (if same conversation)    â”‚
â”‚                            + 0.4 (if query intent matches)  â”‚
â”‚                            + importance/100 * 0.25          â”‚
â”‚                            + confidence/100 * 0.1           â”‚
â”‚                            + 0.1 per keyword match          â”‚
â”‚                                                              â”‚
â”‚    3. Diversity Score = 1.0                                 â”‚
â”‚                       - 0.1 (if same type as previous)      â”‚
â”‚                       - 0.2 per keyword overlap             â”‚
â”‚                                                              â”‚
â”‚    4. Final Score = base_score * diversity_score            â”‚
â”‚                   + contextual_relevance * 0.3              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Lane Filtering (5ms)                               â”‚
â”‚                                                              â”‚
â”‚  CANON memories (facts):                                    â”‚
â”‚    âœ… MUST have lane = 'CANON'                              â”‚
â”‚    âœ… MUST have confidence >= 60                            â”‚
â”‚    âœ… Always retrieved                                      â”‚
â”‚                                                              â”‚
â”‚  RUMOR memories (bullshit):                                 â”‚
â”‚    ðŸŽ­ Only in "Theater Zone":                               â”‚
â”‚       - PODCAST mode                                         â”‚
â”‚       - STREAMING mode                                       â”‚
â”‚       - Chaos > 70                                           â”‚
â”‚    ðŸŽ­ Max confidence: 40                                    â”‚
â”‚    ðŸŽ­ Limited to 3 rumors                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: Knowledge Gap Detection (10ms)                     â”‚
â”‚                                                              â”‚
â”‚  If keywords don't match retrieved memories:                â”‚
â”‚    â†’ "You don't know nuttin' about: X, Y, Z"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 7: Context Pruning (20ms)                             â”‚
â”‚  Remove redundant info already in conversation history      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FINAL RESULT (sent to Gemini)                              â”‚
â”‚                                                              â”‚
â”‚  [CANON MEMORIES]                                            â”‚
â”‚  - Sal is a butcher from Newark (confidence: 95)           â”‚
â”‚  - Sal taught me about meat cuts (confidence: 80)          â”‚
â”‚                                                              â”‚
â”‚  [RUMORS] (if Theater Zone)                                 â”‚
â”‚  - Sal once wrestled a bear (confidence: 30)               â”‚
â”‚  - Sal secretly runs the mafia (confidence: 35)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TOTAL TIME: ~710ms before Nicky starts talking
```

---

## ðŸ“Š The Scoring System Breakdown

You currently have **EIGHT** different scoring mechanisms. Here's what each one does:

### 1. **Semantic Similarity** (Vector Cosine Distance)
- **Source:** Vector embedding search via Gemini
- **Range:** 0.0 to 1.0 (boosted to 1.2 for semantic matches)
- **Cost:** 200ms + Gemini API call
- **Used in:** Primary ranking

### 2. **Keyword Match Score**
- **Source:** SQL `LIKE` queries
- **Range:** 0.7 (default for keyword matches)
- **Cost:** 50ms
- **Used in:** Primary ranking (if not found in semantic)

### 3. **Importance** (1-100)
- **Source:** Memory field, set during extraction based on AI assessment
- **Scale (as of v1.9.0):**
  - 1-25: Minor details, trivial info
  - 26-45: Standard facts, common details
  - 46-60: Notable facts (MOST facts should be here)
  - 61-75: Important facts, key traits
  - 76-100: CRITICAL ONLY - core identity facts (use sparingly!)
- **Contribution:** `importance * 0.005` (max +0.5) - reduced in v1.8.0
- **Used in:** Base score, contextual relevance

### 4. **Confidence** (1-100)
- **Source:** Memory field, set during extraction and boosted on duplicates
- **Tiers (as of v1.9.0):**
  - 1-59: Low confidence, unverified content
  - 60-75: Standard auto-extracted content
  - 76-85: Boosted/frequently confirmed facts (auto-ceiling for non-protected)
  - 86-99: Human-verified content (manual only)
  - 100: Protected core identity facts only
- **Contribution:** `confidence * 0.001` (max +0.1)
- **Used in:** Base score, contextual relevance, HARD FILTER (must be >= 60 for CANON)

### 5. **Contextual Relevance** (calculated)
- **Factors:**
  - Same conversation: +0.5
  - Query intent matches memory type: +0.4
  - Importance contribution: +0.25 max
  - Confidence contribution: +0.1 max
  - Keyword matches: +0.1 per match (max +0.3)
- **Range:** 0.0 to 1.0
- **Used in:** Final score calculation

### 6. **Diversity Score** (penalty multiplier)
- **Factors:**
  - Same type as already-selected memory: -0.1
  - Keyword overlap: -0.2 per overlap
- **Range:** 0.0 to 1.0
- **Used in:** Final score calculation (as multiplier)

### 7. **Retrieval Count**
- **Source:** How many times this memory has been retrieved
- **Used in:** Vector search ranking formula only: `importance / (1 + retrievalCount / 50)`
- **Effect:** Slightly penalizes frequently-retrieved memories

### 8. **Success Rate** (0-100)
- **Source:** How useful this memory has been in conversations
- **Used in:** **NOWHERE!** Stored but never queried

---

## ðŸ”¥ The Problem: Too Complex

### What's Actually Happening Every Message:

```
Step                          Time      API Calls    Database Queries
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Extract keywords              10ms      0            0
Enhance keywords              50ms      0            1 (recent messages)
Generate embedding           200ms      1 (Gemini)   0
Semantic search              100ms      0            1 (vector search)
Keyword search                50ms      0            1
Fetch podcast memories        50ms      0            1
Search documents              50ms      0            1
Fetch lore                    50ms      0            1
Search training examples     100ms      0            1
Calculate relevance           10ms      0            0
Calculate diversity           10ms      0            0
Detect knowledge gaps         10ms      0            0
Prune context                 20ms      0            0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                      ~710ms      1            7 queries
```

### Unused Complexity:

1. **Success Rate** - Tracked but never used
2. **Quality Score** - In schema but never queried
3. **Temporal Context** - Stored but not used for scoring
4. **Cluster ID** - Not actively used
5. **Relationships array** - Stored but not traversed

### Redundant Scoring:

- **Importance** is used 3 times (base score, contextual relevance, vector ranking)
- **Confidence** is used 3 times (base score, contextual relevance, hard filter)
- Semantic and keyword searches overlap significantly

---

## ðŸ’¡ Example: What Gets Retrieved

Given this message: **"Hey Nicky, what's your favorite pasta?"**

### Memories in Database (3,679 total):

```
ID   | Content                                    | Lane   | Conf | Importance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
M1   | Nicky's favorite pasta is carbonara       | CANON  | 82   | 75
M2   | Nicky HATES cream in carbonara            | CANON  | 78   | 65
M3   | Carbonara must have guanciale, not bacon  | CANON  | 75   | 55
M4   | Nicky once threw a plate at someone       | RUMOR  | 30   | 40
     | who used cream
M5   | Nicky's grandmother taught him to cook    | CANON  | 70   | 60
M6   | Penne is acceptable but rigatoni is best  | CANON  | 65   | 45
M7   | Nicky's favorite color is red             | CANON  | 72   | 35
M8   | Nicky has 200 hours in Dead by Daylight   | CANON  | 80   | 50

Note: As of v1.9.0, confidence values cap at 85 for auto-extracted content.
Values 86+ are reserved for human-verified facts, 100 for protected facts only.
```

### Step 1: Keyword Extraction
```
Keywords: ["nicky", "favorite", "pasta"]
```

### Step 2: Hybrid Search (300ms)

**Semantic Search** (generates embedding, searches vectors):
- M1 (similarity: 0.92) - Direct match!
- M2 (similarity: 0.78) - Related to pasta
- M3 (similarity: 0.75) - Related to pasta
- M5 (similarity: 0.65) - Related (cooking)
- M6 (similarity: 0.60) - Related (pasta type)

**Keyword Search** (SQL LIKE):
- M1 (matches: "pasta")
- M2 (matches: "carbonara", context of pasta)
- M3 (matches: "carbonara", context of pasta)

### Step 3: Scoring

**M1: Carbonara is favorite**
```
Base score = 0.92 * 1.2 + 75 * 0.005 + 82 * 0.001
           = 1.104 + 0.375 + 0.082
           = 1.561

Contextual relevance = 0.5 (base)
                     + 0.4 (PREFERENCE matches "favorite" intent)
                     + 0.1875 (importance contribution: 75/100 * 0.25)
                     + 0.082 (confidence contribution)
                     + 0.3 (3 keyword matches)
                     = 1.0 (capped)

Diversity = 1.0 (first result, no penalty)

Final score = 1.561 * 1.0 + 1.0 * 0.3 = 1.861
```

**M7: Favorite color is red**
```
Base score = 0.55 * 1.2 + 35 * 0.005 + 72 * 0.001
           = 0.66 + 0.175 + 0.072
           = 0.907

Contextual relevance = 0.5 + 0.1 (keyword: "favorite") = 0.6

Diversity = 1.0 - 0.1 (same type: PREFERENCE) = 0.9

Final score = 0.907 * 0.9 + 0.6 * 0.3 = 0.996
```

**M8: Dead by Daylight hours**
```
Base score = 0.45 * 1.2 + 50 * 0.005 + 80 * 0.001
           = 0.54 + 0.25 + 0.08
           = 0.87

Contextual relevance = 0.5 (base only, no matches)

Diversity = 1.0 - 0.2 (no type match but keyword overlap with "Nicky")

Final score = 0.87 * 0.8 + 0.5 * 0.3 = 0.846
```

**Note (v1.9.0):** With the reduced importance multiplier (0.005 vs old 0.1),
semantic similarity now dominates scoring. High-importance but irrelevant
memories no longer overwhelm the results.

### Step 4: Filtering

**CANON Filter** (confidence >= 60):
- âœ… M1 (82) â†’ KEEP
- âœ… M2 (78) â†’ KEEP
- âœ… M3 (75) â†’ KEEP
- âŒ M4 (30) â†’ RUMOR lane, skip in normal chat
- âœ… M5 (70) â†’ KEEP
- âœ… M6 (65) â†’ KEEP
- âœ… M7 (72) â†’ KEEP (but low relevance score)
- âœ… M8 (80) â†’ KEEP (but low relevance score)

**Sort by Final Score:**
1. M1 (1.861) - Best semantic match + importance
2. M2 (similar high score) - Related pasta content
3. M3 (similar high score) - Related pasta content
4. M5 (moderate) - Cooking context
5. M6 (moderate) - Pasta types
6. M7 (0.996) - Wrong topic, but still passed filter
7. M8 (0.846) - Irrelevant, but now properly ranked LOW due to v1.8.0 fix!

### Step 5: Returned to Gemini (top 8)

```
[CANON MEMORIES]
- Nicky's favorite pasta is carbonara
- Nicky HATES cream in carbonara
- Carbonara must have guanciale, not bacon
- Nicky's grandmother taught him to cook
- Penne is acceptable but rigatoni is best
- Nicky's favorite color is red (low relevance, may be filtered by limit)
- Nicky has 200 hours in Dead by Daylight (lowest score, likely filtered)
```

**v1.8.0+ Improvement:** M8 (Dead by Daylight) now ranks LAST because semantic
similarity dominates scoring. The importance multiplier fix (0.1 â†’ 0.005)
prevents high-importance but irrelevant memories from overwhelming results.

---

## ðŸŽ¯ Key Insights

### What's Working:
1. **Semantic search** finds contextually related memories well
2. **Keyword search** catches exact matches
3. **Lane filtering** (CANON vs RUMOR) is a good concept
4. **Confidence threshold** prevents low-quality memories

### What Was Broken (Fixed in v1.8.0 & v1.9.0):
1. ~~**Importance and confidence too heavily weighted**~~ âœ… FIXED v1.8.0: Importance multiplier reduced from 0.1 to 0.005
2. ~~**Confidence inflation**~~ âœ… FIXED v1.9.0: Auto-ceiling at 85, slower boost growth (+3 vs +10)
3. ~~**Importance always increasing**~~ âœ… FIXED v1.9.0: Now uses weighted average instead of MAX
4. ~~**Patch notes importance: 850**~~ âœ… FIXED v1.9.0: Now correctly 35-65 on 1-100 scale

### Remaining Issues:
1. **Too many scoring layers** - 8 different mechanisms that overlap
2. **Retrieval count** is barely used (only in vector ranking)
3. **Success rate** is tracked but never used
4. **Contextual relevance calculation is expensive** (10ms Ã— 50 memories = 500ms wasted)
5. **Diversity scoring** is applied AFTER retrieval, should be during

### Performance Issues:
1. **Embedding generation on EVERY message** (200ms + API cost)
2. **7 database queries** per message
3. **50+ memories scored individually** (contextual relevance Ã— diversity)
4. **No caching** for common queries like "what's your favorite X"

---

## ðŸ› ï¸ Simplification Proposals

### Option 1: Keep It Simple
```typescript
// ONE score: relevance
score = semantic_similarity * 0.7 + keyword_match * 0.3
      + (importance / 100) * 0.2

// ONE filter: confidence
if (memory.confidence >= 70 && memory.lane === 'CANON') {
  return memory;
}
```

### Option 2: Trust the Vector Search
```typescript
// Use ONLY semantic search
const results = await vectorSearch(queryEmbedding, limit=10);

// Simple filter
return results.filter(m => m.confidence >= 70);
```

### Option 3: Cache Common Patterns
```typescript
// Cache embeddings for your common phrases
const COMMON_QUERIES = {
  "what's your favorite pasta": <cached_embedding>,
  "tell me a story": <cached_embedding>,
  "what happened in episode X": <cached_embedding>
};
```

---

## ðŸ“ Recommendations

1. **Remove unused fields:**
   - `successRate` (never queried)
   - `qualityScore` (never queried)
   - `temporalContext` (not used for scoring)

2. **Simplify scoring:**
   - Keep: `semantic_similarity`, `confidence`
   - Maybe keep: `importance` (but reduce weight)
   - Remove: `retrievalCount`, `diversityScore`, `contextualRelevance` (too complex)

3. **Optimize performance:**
   - Cache embeddings for your common phrases
   - Run keyword search FIRST, only do semantic if needed
   - Reduce parallel queries from 7 to 3-4

4. **Trust your data:**
   - If a memory is in the database, it's probably relevant
   - Let the vector search do its job
   - Don't over-engineer the ranking

---

## ðŸ§ª Testing This Yourself

To see this in action, add logging to `contextBuilder.ts`:

```typescript
// In retrieveContextualMemories() around line 220
selectedResults.forEach((result, i) => {
  console.log(`[${i+1}] Score: ${result.finalScore.toFixed(2)} | "${result.content}"`);
});
```

Then send Nicky a message and watch the console. You'll see exactly what's being retrieved and how it's scored.
